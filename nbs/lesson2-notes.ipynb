{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Notes for lesson 2\n",
    "https://www.youtube.com/watch?v=e3aM6XTekJc  \n",
    "These notes start from about 23 minutes into the video - had watched the previous stuff before\n",
    "Notes are also available here in the wiki- http://wiki.fast.ai/index.php/Lesson_2_Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Spends some time talking about how to create submission files for kaggle  \n",
    "vgg.test uses predict_generator from Keras - predict_generator can actually give you probabilities \n",
    "Discusses an Ipython utility called FileLink - that helps you create a link that you can use to submit files  \n",
    "\n",
    "### What is LogLoss?  \n",
    "For two classes, this is $l(y, p) = -y\\log(p) + (1-y)\\log(1-p)$ where $p$ is the probability that our model predicts the a value of 1 and $y$ is the correct value. When we have a highly confident but wrong prediction, we get penalized massively. \n",
    "\n",
    "In general, with many images/data points this becomes\n",
    "$L(y,p) = -\\frac{1}{N} \\sum_{i = 1}^{N} l(y_i, p_i)$\n",
    "\n",
    "Jeremy looks at the examples from the validation set where the model got the correct labels, and cases where the model is quite unsure and also looks at cases where the model is highly confident but is actually wrong. \n",
    "\n",
    "You need to be careful with probabilities that come out from deep learning networks - they obey the rules of probability but cannot really be interpreted in a statistical sense. \n",
    "\n",
    "\n",
    "Talks about applying the same image classification to the state farm competition \n",
    "\n",
    "## Finetuning\n",
    "Reference: [Visualizing and understanding convolutional networks](https://arxiv.org/abs/1311.2901) Matthew D. Zeiler and Rob Fergus\n",
    "\n",
    "Shows various filters that get triggered by images \n",
    "Vgg network has 16 layers - we can use the earlier filters (the ones that figure out diagonal lines, circles, edges etc) and then the layers on the learn higher-level functions\n",
    "\n",
    "\n",
    "### Neural networks\n",
    "\n",
    "Jeremy uses matrix multiply in excel to describe a neural network\n",
    "Start neural networks with activations that are close to the output scale. \n",
    "\n",
    "Xavier initialization - the weights are drawn from a normal distribution such that the variance depends only on the input and output dimensions \n",
    "$var(W) = \\frac{2}{n_{in} + n_{out}}$\n",
    "\n",
    "### Creating a linear model \n",
    "From the notebook - \n",
    "Showcases an optimization problem using gradient descent. \n",
    "Very nicely shows how you can animate how the learning proceeds - try applying this to the perceptron code that you wrote. \n",
    "\n",
    "Talks about how deep learning does not have local minima because of the large number of parameters. Very low probablility of finding local minima in high dimensional space - 600 million dimensions! \n",
    "\n",
    "Probability of a local minima - $2^{600000000}$ - do not understand this at allw\n",
    "\n",
    "### Linear model in Keras\n",
    "\n",
    "Sequential (multiple layer networks) in keras \n",
    "and showcases a linear model using keras and then optimizes it using SGD\n",
    "\n",
    "Linear model that takes the output from the imagenet (1000 predictions) as input to a linear model that has two outputs - cat and dog \n",
    "\n",
    "#### Saving numpy array \n",
    "bcolz library\n",
    "\n",
    "#### Onehot encoding \n",
    "Lets you do deep learning very easily with categorical variables - he has a helper function to do this\n",
    "\n",
    "#### RMSprop for the optimization \n",
    "Better than using SGD? \n",
    "\n",
    "Deep learning - activation function is non-linear \n",
    "relu - max(0,x) for every weight and introduces non-linearity\n",
    "\n",
    "vgg.model.summary() - summarizes the model that we have so far\n",
    "\n",
    "vgg.finetune - removes the last layer and adds a dense layer instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
