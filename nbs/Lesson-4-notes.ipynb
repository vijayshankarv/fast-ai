{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on lesson 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy is showing convolutional layers using excel - Fairly straightforward might be a good idea to work out the dimensions for filter sizes and image sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic learning rates and other ways to improve gradient descent\n",
    "\n",
    "#### Momentum \n",
    "\n",
    "What is the method? \n",
    "Add an additional vector of the average of the previous gradients? That way, you can still kind of go in the best direction and not be influenced too much by individual samples. But then this is not really a learning rate. \n",
    "\n",
    "What is the problem with momentum? \n",
    "There is no adaptivity? Can overshoot still? \n",
    "\n",
    "#### Adagrad\n",
    "Divide the learning rate by the sqrt(sum of the squares) of the gradient from the previous batch? (but only in a mini batch - last few examples)\n",
    "This will give different learning rates for different parameters. \n",
    "Intuitively think of this as \n",
    "divide the \n",
    "\n",
    "#### RMSProp \n",
    "Divide the learning rate by the sqrt(sum of the squares) of the gradient? (but only in a mini batch - last few examples)\n",
    "This will give different learning rates for different parameters. \n",
    "Intuitively think of this as \n",
    "divide the \n",
    "\n",
    "#### Adam \n",
    "\n",
    "#### AdamAnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State farm \n",
    "\n",
    "\n",
    "Just a couple of dense layers at first with data augmentation - rotation, shearing, translation, channel shifting etc\n",
    "### Imagenet\n",
    "Then tries vgg layer and then retrains only the dense layers - not sure if he trains any of the other layers \n",
    "\n",
    "### Pseudo-labeling \n",
    "\n",
    "Using labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Collaborative Filtering\n",
    "\n",
    "Amazing how well it works starting from random inputs \n",
    "\n",
    "Five feature weights for each movie and for each user. Then using a set of ratings for movies, you can predict what the best weights for each movie and each user must be. Adding biases to each user and each movie can help make better predictions. Deep learning seemingly does a lot better than this 5x5 collaborative filtering algorithm.\n",
    "\n",
    "Excel has a gradient descent solver? Excel is the bomb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
