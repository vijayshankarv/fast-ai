{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Notes for Lesson 5\n",
    " \n",
    "The first few minutes were spent talking about batch normalization and how to implement it in vgg (maybe only for the dense layers and not for the convolutional layers) \n",
    "\n",
    "Look into this more carefully. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Collaborative filtering \n",
    "\n",
    "Continued with last week's stuff about collaborative filtering (implement this) and then showed some of the latent vectors that the model had learnt about the movies and how using PCA (gives something like the eigenvalues of the latent factor matrix?) one could make sense of what each of these latent factors were trying to identify. In the movie case, there was an axis about happy vs violent movies and another about action versus dialogue heavy movies and another which was hollywood blockbusters vs indie movies. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Keras functional api\n",
    "\n",
    "You can create models by thinking of them as functions acting on inputs. These can be combined together.  \n",
    "\n",
    "Embedding layers can help us create more exotic models customize them to different. You can also add different inputs to various layers. \n",
    "Change your models to use the functional api to get a better hang of it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### NLP using Imdb dataset\n",
    "\n",
    "Sentiment analysis? Keeps only 5000 words and then \n",
    "Paper and dataset - http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Convolutional neural networks for this - 1d don't understand this yet. \n",
    "\n",
    "Dropout in both embedding and in the dense network. \n",
    "\n",
    "No need to pass along pre-trained networks in NLP, but we just need pre-trained word embeddings or word vectors. Two recommended sources - [Glove](https://nlp.stanford.edu/projects/glove/) and [word2vec](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "These word embeddings are created in such a way as to capture the relationships between different words - closer words occur together and are similar whereas words farther away are unrelated to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove\n",
    "\n",
    "Many different word embeddings from different sources and then the words are converted to vectors of say 50 dimensions. \n",
    "\n",
    "#### Unsupervised learning \n",
    "The embeddings are generated by trying to figure out which sequence of say 11 words is in fact real when compared to one where the middle word is replaced by a random word. This is how it is done in word2vec\n",
    "\n",
    "#### T-SNE\n",
    "Way to visualize higher-dimensional relations in two dimensions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs\n",
    "\n",
    "#### Examples\n",
    "Attentional models from streetview \n",
    "Swiftkey - uses neural networks in their keyboards \n",
    "Random $\\LaTeX$  Andrej Karpathy\n",
    "\n",
    "#### What is it? \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
